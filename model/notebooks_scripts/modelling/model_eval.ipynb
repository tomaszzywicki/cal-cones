{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef2b322",
   "metadata": {},
   "source": [
    "## Evaluation of our FoooDetectionModel pipeline (detection + classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a95f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from FoodDetection import ClassificationModelManager, FoodDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eb6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clsModelsDict = ({\n",
    "    \"meat\": \"classification_models/YOLO/meat.pt\",\n",
    "    \"vegetable\": \"classification_models/YOLO/vegetable.pt\",\n",
    "    \"fruit\": \"classification_models/YOLO/fruit.pt\",\n",
    "    \"cheese-dairy\" : \"classification_models/YOLO/cheese-dairy.pt\",\n",
    "    \"bread-pasta-grains\": \"classification_models/YOLO/bread-pasta-grains.pt\",\n",
    "    \"nuts-seeds\": \"classification_models/YOLO/nuts-seeds.pt\",\n",
    "    \"misc\": \"classification_models/YOLO/misc.pt\",\n",
    "})\n",
    "\n",
    "model = FoodDetectionModel(\n",
    "    detection_model_path=\"runs/detect/yolo_det_v4_m/weights/best.pt\",\n",
    "    classification_config=clsModelsDict,\n",
    "    detection_id_to_name=\"pipeline_data/dicts/detect_classes_v4.json\",\n",
    "    det_to_cls_group=\"pipeline_data/dicts/det_to_cls_groups.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, hamming_loss, jaccard_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_multilabel_detection(model, val_img_dir, binary_labels_val_dir, detect_dict_path, conf_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Runs multilabel evaluation for a detection model.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(detect_dict_path, \"r\") as f:\n",
    "        detection_dict = json.load(f)\n",
    "    num_classes = len(detection_dict)\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    print(f\"Starting evaluation of {len(os.listdir(val_img_dir))} files...\")\n",
    "\n",
    "    img_files = [f for f in os.listdir(val_img_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "    bar_format = \"{l_bar}{bar} {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "\n",
    "    for file_name in tqdm(\n",
    "        img_files,\n",
    "        desc=\"Evaluating\",\n",
    "        bar_format=bar_format,\n",
    "        leave=True,\n",
    "    ):\n",
    "\n",
    "        img_path = os.path.join(val_img_dir, file_name)\n",
    "\n",
    "        # Run model\n",
    "        output = model.run(\n",
    "            image_path=img_path,\n",
    "            conf_threshold=conf_threshold,\n",
    "            det_imgsz=800,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Load ground-truth labels\n",
    "        y_true_file = os.path.join(\n",
    "            binary_labels_val_dir,\n",
    "            file_name.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            with open(y_true_file, \"r\") as f:\n",
    "                y_true = [int(x) for x in f.read().strip().split()]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "        # Extract unique predicted classes\n",
    "        unique_pred_classes = set()\n",
    "        for det in output:\n",
    "            unique_pred_classes.add(det[\"pred_class_id\"])\n",
    "\n",
    "        # Create binary prediction vector\n",
    "        y_pred = [1 if i in unique_pred_classes else 0 for i in range(num_classes)]\n",
    "\n",
    "        all_y_true.append(y_true)\n",
    "        all_y_pred.append(y_pred)\n",
    "\n",
    "    Y_true_np = np.array(all_y_true)\n",
    "    Y_pred_np = np.array(all_y_pred)\n",
    "\n",
    "    print(f\"Processed {len(Y_true_np)} samples.\")\n",
    "\n",
    "    macro_f1 = f1_score(Y_true_np, Y_pred_np, average=\"macro\", zero_division=0)\n",
    "    micro_f1 = f1_score(Y_true_np, Y_pred_np, average=\"micro\", zero_division=0)\n",
    "\n",
    "    samples_f1 = f1_score(Y_true_np, Y_pred_np, average=\"samples\", zero_division=0)\n",
    "\n",
    "    micro_prec = precision_score(Y_true_np, Y_pred_np, average=\"micro\", zero_division=0)\n",
    "    micro_rec = recall_score(Y_true_np, Y_pred_np, average=\"micro\", zero_division=0)\n",
    "    h_loss = hamming_loss(Y_true_np, Y_pred_np)\n",
    "    j_score = jaccard_score(Y_true_np, Y_pred_np, average=\"samples\", zero_division=0)\n",
    "\n",
    "    print(\"\\n--- Multi-label Classification Metrics ---\")\n",
    "    print(f\"Macro F1:   {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1:   {micro_f1:.4f}\")\n",
    "    print(f\"Samples F1:     {samples_f1:.4f}\")\n",
    "    print(f\"Precision: {micro_prec:.4f}\")\n",
    "    print(f\"Recall:  {micro_rec:.4f}\")\n",
    "    print(f\"Hamming:    {h_loss:.4f}\")\n",
    "    print(f\"Jaccard:    {j_score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"samples_f1\": samples_f1,\n",
    "        \"precision\": micro_prec,\n",
    "        \"Recall\": micro_rec,\n",
    "        \"hamming_loss\": h_loss,\n",
    "        \"jaccard_score\": j_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf801cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics with confidence threshold 0.3:\n",
      "Starting evaluation of 3599 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████ 3599/3599 [03:01<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3599 samples.\n",
      "\n",
      "--- Multi-label Classification Metrics ---\n",
      "Macro F1:   0.7303\n",
      "Micro F1:   0.7433\n",
      "Samples F1:     0.7238\n",
      "Precision: 0.7591\n",
      "Recall:  0.7282\n",
      "Hamming:    0.0073\n",
      "Jaccard:    0.6883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation metrics with confidence threshold 0.3:\")\n",
    "metrics = evaluate_multilabel_detection(\n",
    "    model=model,\n",
    "    val_img_dir=\"data/dataset_v4/val/images\",\n",
    "    binary_labels_val_dir=\"data/dataset_v4/val/binary_labels\",\n",
    "    detect_dict_path=\"pipeline_data/dicts/detect_classes_v4.json\",\n",
    "    conf_threshold=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d972b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics with confidence threshold 0.2:\n",
      "Starting evaluation of 3599 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████ 3599/3599 [03:06<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3599 samples.\n",
      "\n",
      "--- Multi-label Classification Metrics ---\n",
      "Macro F1:   0.7261\n",
      "Micro F1:   0.7373\n",
      "Samples F1:     0.7337\n",
      "Precision: 0.7181\n",
      "Recall:  0.7575\n",
      "Hamming:    0.0079\n",
      "Jaccard:    0.6929\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation metrics with confidence threshold 0.2:\")\n",
    "metrics = evaluate_multilabel_detection(\n",
    "    model=model,\n",
    "    val_img_dir=\"data/dataset_v4/val/images\",\n",
    "    binary_labels_val_dir=\"data/dataset_v4/val/binary_labels\",\n",
    "    detect_dict_path=\"pipeline_data/dicts/detect_classes_v4.json\",\n",
    "    conf_threshold=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba04819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics with confidence threshold 0.15:\n",
      "Starting evaluation of 3599 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████ 3599/3599 [02:50<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3599 samples.\n",
      "\n",
      "--- Multi-label Classification Metrics ---\n",
      "Macro F1:   0.7181\n",
      "Micro F1:   0.7288\n",
      "Samples F1:     0.7339\n",
      "Precision: 0.6901\n",
      "Recall:  0.7721\n",
      "Hamming:    0.0084\n",
      "Jaccard:    0.6893\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation metrics with confidence threshold 0.15:\")\n",
    "metrics = evaluate_multilabel_detection(\n",
    "    model=model,\n",
    "    val_img_dir=\"data/dataset_v4/val/images\",\n",
    "    binary_labels_val_dir=\"data/dataset_v4/val/binary_labels\",\n",
    "    detect_dict_path=\"pipeline_data/dicts/detect_classes_v4.json\",\n",
    "    conf_threshold=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8d0acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics with confidence threshold 0.4:\n",
      "Starting evaluation of 3599 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████ 3599/3599 [02:42<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3599 samples.\n",
      "\n",
      "--- Multi-label Classification Metrics ---\n",
      "Macro F1:   0.7291\n",
      "Micro F1:   0.7422\n",
      "Samples F1:     0.7053\n",
      "Precision: 0.7911\n",
      "Recall:  0.6991\n",
      "Hamming:    0.0071\n",
      "Jaccard:    0.6735\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation metrics with confidence threshold 0.4:\")\n",
    "metrics = evaluate_multilabel_detection(\n",
    "    model=model,\n",
    "    val_img_dir=\"data/dataset_v4/val/images\",\n",
    "    binary_labels_val_dir=\"data/dataset_v4/val/binary_labels\",\n",
    "    detect_dict_path=\"pipeline_data/dicts/detect_classes_v4.json\",\n",
    "    conf_threshold=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6e9f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics with confidence threshold 0.5:\n",
      "Starting evaluation of 3599 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████ 3599/3599 [02:40<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3599 samples.\n",
      "\n",
      "--- Multi-label Classification Metrics ---\n",
      "Macro F1:   0.7196\n",
      "Micro F1:   0.7340\n",
      "Samples F1:     0.6826\n",
      "Precision: 0.8144\n",
      "Recall:  0.6681\n",
      "Hamming:    0.0070\n",
      "Jaccard:    0.6529\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation metrics with confidence threshold 0.5:\")\n",
    "metrics = evaluate_multilabel_detection(\n",
    "    model=model,\n",
    "    val_img_dir=\"data/dataset_v4/val/images\",\n",
    "    binary_labels_val_dir=\"data/dataset_v4/val/binary_labels\",\n",
    "    detect_dict_path=\"pipeline_data/dicts/detect_classes_v4.json\",\n",
    "    conf_threshold=0.5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
